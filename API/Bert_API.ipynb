{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9xwzy6WT7mfQ"
   },
   "outputs": [],
   "source": [
    "#set up environment\n",
    "import os, torch, pickle, warnings, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from tqdm import tqdm, trange\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from time import sleep\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "################################### Define functions ##########################\n",
    "def npoclass(inputs, gpu_core=True, n_jobs=4, model_path=None, ntee_type='bc'):\n",
    "    \n",
    "    # Set the seed value all over the place to make this reproducible.\n",
    "    seed_val = 42\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "\n",
    "    # Check model files.\n",
    "    if ntee_type=='bc' and model_path==None:\n",
    "        raise ValueError(\"Make sure model files/path are correct. Please download from https://jima.me/open/npoclass_model_bc.zip, unzip, and specifiy model_path (default set to None).\")\n",
    "    if ntee_type=='mg' and model_path==None:\n",
    "        raise ValueError(\"Make sure model files/path are correct. Please download from https://jima.me/open/npoclass_model_mg.zip, unzip, and specifiy model_path (default set to None).\")\n",
    "        \n",
    "    # Check ntee type.\n",
    "    if ntee_type=='bc':\n",
    "        le_file_name='le_broad_cat.pkl'\n",
    "    elif ntee_type=='mg':\n",
    "        le_file_name='le_major_group.pkl'\n",
    "    else:\n",
    "        raise ValueError(\"ntee_type must be 'bc' (broad category) or 'mg' (major group)\")\n",
    "\n",
    "    # Read model and label encoder, if not read.\n",
    "    global model_loaded, tokenizer_loaded, label_encoder\n",
    "    try:\n",
    "        assert model_loaded\n",
    "        assert tokenizer_loaded\n",
    "        assert label_encoder\n",
    "    except:\n",
    "        #load a pretrained model and tokenizer.\n",
    "        model_loaded = BertForSequenceClassification.from_pretrained(model_path)\n",
    "        tokenizer_loaded = BertTokenizer.from_pretrained(model_path)\n",
    "        # Read label encoder.\n",
    "        with open(model_path+le_file_name, 'rb') as label_encoder_pkl:\n",
    "            label_encoder = pickle.load(label_encoder_pkl)\n",
    "    \n",
    "    # Select acceleration method.\n",
    "    if gpu_core==True and torch.cuda.is_available():\n",
    "        print('There are %d GPU(s) available.' % torch.cuda.device_count(), 'Using GPU:',torch.cuda.get_device_name(0))\n",
    "        torch.cuda.manual_seed_all(seed_val)\n",
    "        device = torch.device('cuda')\n",
    "        model_loaded.cuda()\n",
    "    else:\n",
    "        print('No GPU acceleration available or gpu_core=False, using CPU.')\n",
    "        device = torch.device('cpu')\n",
    "        model_loaded.cpu()\n",
    "    print('Encoding inputs ...')\n",
    "    sleep(.5) # Pause a second for better printing results.\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    # Encode inputs.\n",
    "    def func_encode_string(text_string):\n",
    "        encoded_dict = tokenizer_loaded.encode_plus(text_string,\n",
    "                                                    add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                                                    max_length = 256,           # Pad & truncate all sentences.\n",
    "                                                    truncation=True,\n",
    "                                                    pad_to_max_length = True,\n",
    "                                                    return_attention_mask = True,   # Construct attn. masks.\n",
    "                                                    return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                                                   )\n",
    "        return encoded_dict\n",
    "    # Encode input string(s).\n",
    "    if type(inputs)==list:\n",
    "        encoded_outputs=Parallel(n_jobs=n_jobs, backend=\"threading\", batch_size='auto', verbose=1)(delayed(func_encode_string)(text_string) for text_string in inputs)\n",
    "        for encoded_output in encoded_outputs:\n",
    "            # Add the encoded sentence to the list.\n",
    "            input_ids.append(encoded_output['input_ids'])\n",
    "            # And its attention mask (simply differentiates padding from non-padding).\n",
    "            attention_masks.append(encoded_output['attention_mask'])\n",
    "    if type(inputs)==str:\n",
    "        encoded_outputs=func_encode_string(inputs)\n",
    "        input_ids=[encoded_outputs['input_ids']]\n",
    "        attention_masks=[encoded_outputs['attention_mask']]\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    # Prepare dataloader for efficient calculation.\n",
    "    batch_size = 32\n",
    "    pred_data = TensorDataset(input_ids, attention_masks)\n",
    "    pred_sampler = SequentialSampler(pred_data)\n",
    "    pred_dataloader = DataLoader(pred_data, sampler=pred_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Start prediction.\n",
    "    model_loaded.eval()\n",
    "    logits_all=[]\n",
    "    print('Predicting categories ...')\n",
    "    sleep(.5) # Pause a second for better printing results.\n",
    "    for batch in tqdm(pred_dataloader):\n",
    "        # Add batch to the pre-chosen device\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model_loaded(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits_all+=outputs[0].tolist()\n",
    "\n",
    "    # Calculate probabilities of logitcs.\n",
    "    logits_prob=tf.nn.sigmoid(logits_all).numpy().tolist()\n",
    "    # Find the positions of max values in logits.\n",
    "    logits_max=np.argmax(logits_prob, axis=1)\n",
    "    # Transfer to labels.\n",
    "    logits_labels=label_encoder.inverse_transform(logits_max)\n",
    "    \n",
    "    # Compile results to be returned.\n",
    "    result_list=[]\n",
    "    for list_index in range(0, len(logits_labels)):\n",
    "        result_dict={}\n",
    "        result_dict['recommended']=logits_labels[list_index]\n",
    "        conf_prob=logits_prob[list_index][logits_max[list_index]]\n",
    "        if conf_prob>=.99:\n",
    "            result_dict['confidence']='high (>=.99)'\n",
    "        elif conf_prob>=.95:\n",
    "            result_dict['confidence']='medium (<.99|>=.95)'\n",
    "        else:\n",
    "            result_dict['confidence']='low (<.95)'\n",
    "        prob_dict={}\n",
    "        for label_index in range(0, len(label_encoder.classes_)):\n",
    "            prob_dict[label_encoder.classes_[label_index]]=logits_prob[list_index][label_index]\n",
    "        result_dict['probabilities']=prob_dict\n",
    "        result_list+=[result_dict]\n",
    "        \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available. Using GPU: Quadro RTX 6000\n",
      "Encoding inputs ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=48)]: Using backend ThreadingBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=48)]: Done 104 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=48)]: Done 200 out of 200 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting categories ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:01<00:00,  6.72it/s]\n"
     ]
    }
   ],
   "source": [
    "t=npoclass(['educators service, environment tree protection']*200, gpu_core=True, model_path='../../npoclass_model_bc/', n_jobs=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'recommended': 'II',\n",
       "  'confidence': 'high (>=.99)',\n",
       "  'probabilities': {'I': 0.5053212642669678,\n",
       "   'II': 0.9996891021728516,\n",
       "   'III': 0.7522094249725342,\n",
       "   'IV': 0.6053230166435242,\n",
       "   'IX': 0.2062983214855194,\n",
       "   'V': 0.9766567945480347,\n",
       "   'VI': 0.270598441362381,\n",
       "   'VII': 0.8041079044342041,\n",
       "   'VIII': 0.32034310698509216}},\n",
       " {'recommended': 'II',\n",
       "  'confidence': 'high (>=.99)',\n",
       "  'probabilities': {'I': 0.5053212642669678,\n",
       "   'II': 0.9996891021728516,\n",
       "   'III': 0.7522094249725342,\n",
       "   'IV': 0.6053230166435242,\n",
       "   'IX': 0.2062983214855194,\n",
       "   'V': 0.9766567945480347,\n",
       "   'VI': 0.270598441362381,\n",
       "   'VII': 0.8041079044342041,\n",
       "   'VIII': 0.32034310698509216}},\n",
       " {'recommended': 'II',\n",
       "  'confidence': 'high (>=.99)',\n",
       "  'probabilities': {'I': 0.5053212642669678,\n",
       "   'II': 0.9996891021728516,\n",
       "   'III': 0.7522094249725342,\n",
       "   'IV': 0.6053230166435242,\n",
       "   'IX': 0.2062983214855194,\n",
       "   'V': 0.9766567945480347,\n",
       "   'VI': 0.270598441362381,\n",
       "   'VII': 0.8041079044342041,\n",
       "   'VIII': 0.32034310698509216}}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available. Using GPU: Quadro RTX 6000\n",
      "Encoding inputs ...\n",
      "Predicting categories ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 23.73it/s]\n"
     ]
    }
   ],
   "source": [
    "t=npoclass('educators service, environment tree protection', gpu_core=True, model_path='../../npoclass_model_bc/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'recommended': 'II',\n",
       "  'confidence': 'high (>=.99)',\n",
       "  'probabilities': {'I': 0.5053212642669678,\n",
       "   'II': 0.9996891021728516,\n",
       "   'III': 0.7522097826004028,\n",
       "   'IV': 0.605323076248169,\n",
       "   'IX': 0.20629839599132538,\n",
       "   'V': 0.9766567945480347,\n",
       "   'VI': 0.2705982029438019,\n",
       "   'VII': 0.8041078448295593,\n",
       "   'VIII': 0.3203430771827698}}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available. Using GPU: Quadro RTX 6000\n",
      "Encoding inputs ...\n",
      "Predicting categories ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 31.94it/s]\n"
     ]
    }
   ],
   "source": [
    "t=npoclass('educators service, environment tree protection', gpu_core=True, ntee_type='bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "exec(requests.get('https://raw.githubusercontent.com/ma-ji/npo_classifier/master/API/npoclass.py').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZk4LM6k7mfd"
   },
   "outputs": [],
   "source": [
    "df_UCF_eval=pd.read_pickle('../dataset/UCF/test/df_ucf_test.pkl.gz')\n",
    "df_UCF_eval['input']= df_UCF_eval['TAXPAYER_NAME']+' '+df_UCF_eval['mission_spellchk']+' '+df_UCF_eval['prgrm_dsc_spellchk']\n",
    "\n",
    "# Code as 10 broad categories.\n",
    "broad_cat_dict={'I': ['A'],\n",
    "                'II': ['B'],\n",
    "                'III': ['C', 'D'],\n",
    "                'IV': ['E', 'F', 'G', 'H'],\n",
    "                'V': ['I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'],\n",
    "                'VI': ['Q'],\n",
    "                'VII': ['R', 'S', 'T', 'U', 'V', 'W'],\n",
    "                'VIII': ['X'],\n",
    "                'IX': ['Y'],\n",
    "                'X': ['Z'],\n",
    "               }\n",
    "def ntee2cat(string):\n",
    "    global broad_cat_dict\n",
    "    return [s for s in broad_cat_dict.keys() if string in broad_cat_dict[s]][0]\n",
    "\n",
    "df_UCF_eval['broad_cat']=df_UCF_eval['NTEE1'].apply(ntee2cat)\n",
    "\n",
    "# Create sentence and encoded label lists\n",
    "sentences = df_UCF_eval.input.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available. Using GPU: Quadro RTX 6000\n",
      "Encoding inputs ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=48)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=48)]: Done 104 tasks      | elapsed:   19.4s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-37d13976b9a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnpoclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../../npoclass_model_bc/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mnpoclass\u001b[0;34m(inputs, gpu_core, n_jobs, model_path, ntee_type)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_results=npoclass(sentences, model_path='../../npoclass_model_bc/', n_jobs=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results=npoclass(sentences, model_path='npoclass_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_SqyKYlA7mf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available. Using GPU: Quadro RTX 6000\n",
      "Encoding inputs ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38607/38607 [01:31<00:00, 419.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting categories ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1207/1207 [03:15<00:00,  6.17it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_results=npoclass(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          I     0.9220    0.9170    0.9903    0.9195    0.9530    0.9015      4291\n",
      "         II     0.9145    0.9084    0.9831    0.9114    0.9450    0.8863      6419\n",
      "        III     0.8968    0.9151    0.9947    0.9059    0.9541    0.9030      1861\n",
      "         IV     0.8989    0.8847    0.9874    0.8917    0.9347    0.8646      4329\n",
      "         IX     0.9091    0.9353    0.9957    0.9221    0.9650    0.9257      1701\n",
      "          V     0.9034    0.9176    0.9572    0.9105    0.9372    0.8749     11723\n",
      "         VI     0.6742    0.6835    0.9962    0.6788    0.8252    0.6596       436\n",
      "        VII     0.9047    0.8822    0.9803    0.8933    0.9300    0.8564      6749\n",
      "       VIII     0.8166    0.8352    0.9945    0.8258    0.9114    0.8173      1098\n",
      "\n",
      "avg / total     0.9019    0.9018    0.9776    0.9018    0.9387    0.8749     38607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.metrics import classification_report_imbalanced\n",
    "print(classification_report_imbalanced(y_true=df_UCF_eval.broad_cat, y_pred=[s['recommended'] for s in eval_results], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Bert_API.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
