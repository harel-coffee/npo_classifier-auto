{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9xwzy6WT7mfQ"
   },
   "outputs": [],
   "source": [
    "#set up environment\n",
    "import os, torch, pickle, warnings, random, joblib, math, itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from time import sleep\n",
    "from joblib import Parallel, delayed\n",
    "from tlz import partition_all\n",
    "from multiprocessing import Pool\n",
    "\n",
    "################################### Define functions ##########################\n",
    "def npoclass(inputs, gpu_core=True, model_path=None, ntee_type='bc', n_jobs=4, backend='multiprocessing'):\n",
    "    \n",
    "    # Set the seed value all over the place to make this reproducible.\n",
    "    seed_val = 42\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "\n",
    "    # Check model files.\n",
    "    if ntee_type=='bc' and model_path==None:\n",
    "        raise ValueError(\"Make sure model files/path are correct. Please download from https://jima.me/open/npoclass_model_bc.zip, unzip, and specifiy model_path (default set to None).\")\n",
    "    if ntee_type=='mg' and model_path==None:\n",
    "        raise ValueError(\"Make sure model files/path are correct. Please download from https://jima.me/open/npoclass_model_mg.zip, unzip, and specifiy model_path (default set to None).\")\n",
    "        \n",
    "    # Check ntee type.\n",
    "    if ntee_type=='bc':\n",
    "        le_file_name='le_broad_cat.pkl'\n",
    "    elif ntee_type=='mg':\n",
    "        le_file_name='le_major_group.pkl'\n",
    "    else:\n",
    "        raise ValueError(\"ntee_type must be 'bc' (broad category) or 'mg' (major group)\")\n",
    "\n",
    "    # Read model and label encoder, if not read.\n",
    "    global model_loaded, tokenizer_loaded, label_encoder\n",
    "    try:\n",
    "        assert model_loaded\n",
    "        assert tokenizer_loaded\n",
    "        assert label_encoder\n",
    "    except:\n",
    "        #load a pretrained model and tokenizer.\n",
    "        model_loaded = BertForSequenceClassification.from_pretrained(model_path)\n",
    "        tokenizer_loaded = BertTokenizer.from_pretrained(model_path)\n",
    "        # Read label encoder.\n",
    "        with open(model_path+le_file_name, 'rb') as label_encoder_pkl:\n",
    "            label_encoder = pickle.load(label_encoder_pkl)\n",
    "    \n",
    "    # Select acceleration method.\n",
    "    if gpu_core==True and torch.cuda.is_available():\n",
    "        print('There are %d GPU(s) available.' % torch.cuda.device_count(), 'Using GPU:',torch.cuda.get_device_name(0))\n",
    "        torch.cuda.manual_seed_all(seed_val)\n",
    "        device = torch.device('cuda')\n",
    "        model_loaded.cuda()\n",
    "    else:\n",
    "        print('No GPU acceleration available or gpu_core=False, using CPU.')\n",
    "        device = torch.device('cpu')\n",
    "        model_loaded.cpu()\n",
    "    print('Encoding inputs ...')\n",
    "    sleep(.5) # Pause a second for better printing results.\n",
    "    \n",
    "    # Encode inputs.\n",
    "    global func_encode_string, func_encode_string_batch # Define as global, otherwise cannot pickle or very slow.\n",
    "    def func_encode_string(text_string):\n",
    "        encoded_dict = tokenizer_loaded.encode_plus(text_string,\n",
    "                                                    add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                                                    max_length = 256,           # Pad & truncate all sentences.\n",
    "                                                    truncation=True,\n",
    "                                                    pad_to_max_length = True,\n",
    "                                                    return_attention_mask = True,   # Construct attn. masks.\n",
    "                                                    return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                                                   )\n",
    "        return encoded_dict\n",
    "    def func_encode_string_batch(text_strings):\n",
    "        encoded_dicts=[]\n",
    "        for text_string in text_strings:\n",
    "            encoded_dicts+=[func_encode_string(text_string)]\n",
    "        return encoded_dicts\n",
    "\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    # Encode input string(s).\n",
    "    if type(inputs)==list:\n",
    "        if backend=='multiprocessing': # Multiprocessing is faster than loky in processing large objects.\n",
    "            encoded_outputs=Parallel(n_jobs=n_jobs, backend=\"multiprocessing\", batch_size='auto', verbose=1)(delayed(func_encode_string)(text_string) for text_string in inputs)\n",
    "            for encoded_output in encoded_outputs:\n",
    "                # Add the encoded sentence to the list.\n",
    "                input_ids.append(encoded_output['input_ids'])\n",
    "                # And its attention mask (simply differentiates padding from non-padding).\n",
    "                attention_masks.append(encoded_output['attention_mask'])\n",
    "        elif backend=='sequential':\n",
    "            for text_string in tqdm(inputs):\n",
    "                encoded_output=func_encode_string(text_string)\n",
    "                # Add the encoded sentence to the list.\n",
    "                input_ids.append(encoded_output['input_ids'])\n",
    "                # And its attention mask (simply differentiates padding from non-padding).\n",
    "                attention_masks.append(encoded_output['attention_mask'])\n",
    "        elif backend=='dask':\n",
    "            with joblib.parallel_backend('dask'):\n",
    "                n_jobs=len(client.scheduler_info()['workers']) # Get # works.\n",
    "                string_chunks = partition_all(math.ceil(len(inputs)/n_jobs), inputs)  # Collect into groups of size 1000\n",
    "                encoded_outputs=Parallel(n_jobs=-1, batch_size='auto', verbose=1)(delayed(func_encode_string_batch)(text_strings) for text_strings in string_chunks)\n",
    "                encoded_outputs=itertools.chain(*encoded_outputs)\n",
    "            for encoded_output in encoded_outputs:\n",
    "                # Add the encoded sentence to the list.\n",
    "                input_ids.append(encoded_output['input_ids'])\n",
    "                # And its attention mask (simply differentiates padding from non-padding).\n",
    "                attention_masks.append(encoded_output['attention_mask'])           \n",
    "    if type(inputs)==str:\n",
    "        encoded_output=func_encode_string(inputs)\n",
    "        input_ids=[encoded_output['input_ids']]\n",
    "        attention_masks=[encoded_output['attention_mask']]\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    # Prepare dataloader for efficient calculation.\n",
    "    batch_size = 64\n",
    "    pred_data = TensorDataset(input_ids, attention_masks)\n",
    "    pred_sampler = SequentialSampler(pred_data)\n",
    "    pred_dataloader = DataLoader(pred_data, sampler=pred_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Start prediction.\n",
    "    model_loaded.eval()\n",
    "    logits_all=[]\n",
    "    print('Predicting categories ...')\n",
    "    sleep(.5) # Pause a second for better printing results.\n",
    "    for batch in tqdm(pred_dataloader, mininterval=10):\n",
    "        # Add batch to the pre-chosen device\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model_loaded(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits_all+=outputs[0].tolist()\n",
    "\n",
    "    # Calculate probabilities of logitcs.\n",
    "    logits_prob=tf.nn.sigmoid(logits_all).numpy().tolist()\n",
    "    # Find the positions of max values in logits.\n",
    "    logits_max=np.argmax(logits_prob, axis=1)\n",
    "    # Transfer to labels.\n",
    "    logits_labels=label_encoder.inverse_transform(logits_max)\n",
    "    \n",
    "    # Compile results to be returned.\n",
    "    result_list=[]\n",
    "    for list_index in range(0, len(logits_labels)):\n",
    "        result_dict={}\n",
    "        result_dict['recommended']=logits_labels[list_index]\n",
    "        conf_prob=logits_prob[list_index][logits_max[list_index]]\n",
    "        if conf_prob>=.99:\n",
    "            result_dict['confidence']='high (>=.99)'\n",
    "        elif conf_prob>=.95:\n",
    "            result_dict['confidence']='medium (<.99|>=.95)'\n",
    "        else:\n",
    "            result_dict['confidence']='low (<.95)'\n",
    "        prob_dict={}\n",
    "        for label_index in range(0, len(label_encoder.classes_)):\n",
    "            prob_dict[label_encoder.classes_[label_index]]=logits_prob[list_index][label_index]\n",
    "        result_dict['probabilities']=prob_dict\n",
    "        result_list+=[result_dict]\n",
    "\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "string = requests.get('https://github.com/ma-ji/npo_classifier').text[0:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=npoclass([string]*2000, \n",
    "           gpu_core=True, model_path='../../npoclass_model_bc/', backend='sequential')\n",
    "\n",
    "# >1h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available. Using GPU: Quadro RTX 6000\n",
      "Encoding inputs ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=40)]: Using backend LokyBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done  10 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=40)]: Done 160 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=40)]: Done 410 tasks      | elapsed:   39.3s\n",
      "[Parallel(n_jobs=40)]: Done 760 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=40)]: Done 1210 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=40)]: Done 1760 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=40)]: Done 2000 out of 2000 | elapsed:  2.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting categories ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:09<00:00,  3.41it/s]\n"
     ]
    }
   ],
   "source": [
    "t=npoclass([string]*2000, \n",
    "           gpu_core=True, model_path='../../npoclass_model_bc/', n_jobs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available. Using GPU: Quadro RTX 6000\n",
      "Encoding inputs ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=40)]: Using backend MultiprocessingBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done  10 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=40)]: Done 160 tasks      | elapsed:   18.1s\n",
      "[Parallel(n_jobs=40)]: Done 410 tasks      | elapsed:   44.2s\n",
      "[Parallel(n_jobs=40)]: Done 760 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=40)]: Done 1210 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=40)]: Done 1760 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=40)]: Done 2000 out of 2000 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting categories ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:09<00:00,  3.40it/s]\n"
     ]
    }
   ],
   "source": [
    "t=npoclass([string]*2000, \n",
    "           gpu_core=True, model_path='../../npoclass_model_bc/', n_jobs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available. Using GPU: Quadro RTX 6000\n",
      "Encoding inputs ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend DaskDistributedBackend with 120 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 118 out of 118 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting categories ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:09<00:00,  3.40it/s]\n"
     ]
    }
   ],
   "source": [
    "t=npoclass([string]*2000, \n",
    "           gpu_core=True, model_path='../../npoclass_model_bc/', backend='dask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.140.82.44:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.140.82.44:8787/status' target='_blank'>http://10.140.82.44:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>120</li>\n",
       "  <li><b>Cores: </b>120</li>\n",
       "  <li><b>Memory: </b>240.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.140.82.44:8786' processes=120 threads=120, memory=240.00 GB>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(\"10.140.82.44:8786\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'recommended': 'VII',\n",
       "  'confidence': 'high (>=.99)',\n",
       "  'probabilities': {'I': 0.7859950661659241,\n",
       "   'II': 0.6022496223449707,\n",
       "   'III': 0.6815308928489685,\n",
       "   'IV': 0.7988898158073425,\n",
       "   'IX': 0.46323516964912415,\n",
       "   'V': 0.7328329086303711,\n",
       "   'VI': 0.42189300060272217,\n",
       "   'VII': 0.9975772500038147,\n",
       "   'VIII': 0.2866130769252777}},\n",
       " {'recommended': 'VII',\n",
       "  'confidence': 'high (>=.99)',\n",
       "  'probabilities': {'I': 0.7859950661659241,\n",
       "   'II': 0.6022496223449707,\n",
       "   'III': 0.6815308928489685,\n",
       "   'IV': 0.7988898158073425,\n",
       "   'IX': 0.46323516964912415,\n",
       "   'V': 0.7328329086303711,\n",
       "   'VI': 0.42189300060272217,\n",
       "   'VII': 0.9975772500038147,\n",
       "   'VIII': 0.2866130769252777}},\n",
       " {'recommended': 'VII',\n",
       "  'confidence': 'high (>=.99)',\n",
       "  'probabilities': {'I': 0.7859950661659241,\n",
       "   'II': 0.6022496223449707,\n",
       "   'III': 0.6815308928489685,\n",
       "   'IV': 0.7988898158073425,\n",
       "   'IX': 0.46323516964912415,\n",
       "   'V': 0.7328329086303711,\n",
       "   'VI': 0.42189300060272217,\n",
       "   'VII': 0.9975772500038147,\n",
       "   'VIII': 0.2866130769252777}}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available. Using GPU: Quadro RTX 6000\n",
      "Encoding inputs ...\n",
      "Predicting categories ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 23.73it/s]\n"
     ]
    }
   ],
   "source": [
    "t=npoclass('educators service, environment tree protection', gpu_core=True, model_path='../../npoclass_model_bc/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'recommended': 'II',\n",
       "  'confidence': 'high (>=.99)',\n",
       "  'probabilities': {'I': 0.5053212642669678,\n",
       "   'II': 0.9996891021728516,\n",
       "   'III': 0.7522097826004028,\n",
       "   'IV': 0.605323076248169,\n",
       "   'IX': 0.20629839599132538,\n",
       "   'V': 0.9766567945480347,\n",
       "   'VI': 0.2705982029438019,\n",
       "   'VII': 0.8041078448295593,\n",
       "   'VIII': 0.3203430771827698}}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available. Using GPU: Quadro RTX 6000\n",
      "Encoding inputs ...\n",
      "Predicting categories ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 31.94it/s]\n"
     ]
    }
   ],
   "source": [
    "t=npoclass('educators service, environment tree protection', gpu_core=True, ntee_type='bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "exec(requests.get('https://raw.githubusercontent.com/ma-ji/npo_classifier/master/API/npoclass.py').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#set up environment\\nimport os, torch, pickle, warnings, random, joblib, math, itertools\\nimport pandas as pd\\nimport numpy as np\\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\\nfrom transformers import BertForSequenceClassification, BertTokenizer\\nfrom tqdm import tqdm\\nimport tensorflow as tf\\nwarnings.filterwarnings(\"ignore\")\\nfrom time import sleep\\nfrom joblib import Parallel, delayed\\nfrom tlz import partition_all\\nfrom multiprocessing import Pool\\n\\n################################### Define functions ##########################\\ndef npoclass(inputs, gpu_core=True, model_path=None, ntee_type=\\'bc\\', n_jobs=4, backend=\\'multiprocessing\\'):\\n    \\n    # Set the seed value all over the place to make this reproducible.\\n    seed_val = 42\\n    random.seed(seed_val)\\n    np.random.seed(seed_val)\\n    torch.manual_seed(seed_val)\\n\\n    # Check model files.\\n    if ntee_type==\\'bc\\' and model_path==None:\\n        raise ValueError(\"Make sure model files/path are correct. Please download from https://jima.me/open/npoclass_model_bc.zip, unzip, and specifiy model_path (default set to None).\")\\n    if ntee_type==\\'mg\\' and model_path==None:\\n        raise ValueError(\"Make sure model files/path are correct. Please download from https://jima.me/open/npoclass_model_mg.zip, unzip, and specifiy model_path (default set to None).\")\\n        \\n    # Check ntee type.\\n    if ntee_type==\\'bc\\':\\n        le_file_name=\\'le_broad_cat.pkl\\'\\n    elif ntee_type==\\'mg\\':\\n        le_file_name=\\'le_major_group.pkl\\'\\n    else:\\n        raise ValueError(\"ntee_type must be \\'bc\\' (broad category) or \\'mg\\' (major group)\")\\n\\n    # Read model and label encoder, if not read.\\n    global model_loaded, tokenizer_loaded, label_encoder\\n    try:\\n        assert model_loaded\\n        assert tokenizer_loaded\\n        assert label_encoder\\n    except:\\n        #load a pretrained model and tokenizer.\\n        model_loaded = BertForSequenceClassification.from_pretrained(model_path)\\n        tokenizer_loaded = BertTokenizer.from_pretrained(model_path)\\n        # Read label encoder.\\n        with open(model_path+le_file_name, \\'rb\\') as label_encoder_pkl:\\n            label_encoder = pickle.load(label_encoder_pkl)\\n    \\n    # Select acceleration method.\\n    if gpu_core==True and torch.cuda.is_available():\\n        print(\\'There are %d GPU(s) available.\\' % torch.cuda.device_count(), \\'Using GPU:\\',torch.cuda.get_device_name(0))\\n        torch.cuda.manual_seed_all(seed_val)\\n        device = torch.device(\\'cuda\\')\\n        model_loaded.cuda()\\n    else:\\n        print(\\'No GPU acceleration available or gpu_core=False, using CPU.\\')\\n        device = torch.device(\\'cpu\\')\\n        model_loaded.cpu()\\n    print(\\'Encoding inputs ...\\')\\n    sleep(.5) # Pause a second for better printing results.\\n    \\n    # Encode inputs.\\n    global func_encode_string, func_encode_string_batch # Define as global, otherwise cannot pickle or very slow.\\n    def func_encode_string(text_string):\\n        encoded_dict = tokenizer_loaded.encode_plus(text_string,\\n                                                    add_special_tokens = True, # Add \\'[CLS]\\' and \\'[SEP]\\'\\n                                                    max_length = 256,           # Pad & truncate all sentences.\\n                                                    truncation=True,\\n                                                    pad_to_max_length = True,\\n                                                    return_attention_mask = True,   # Construct attn. masks.\\n                                                    return_tensors = \\'pt\\',     # Return pytorch tensors.\\n                                                   )\\n        return encoded_dict\\n    def func_encode_string_batch(text_strings):\\n        encoded_dicts=[]\\n        for text_string in text_strings:\\n            encoded_dicts+=[func_encode_string(text_string)]\\n        return encoded_dicts\\n\\n    # Tokenize all of the sentences and map the tokens to thier word IDs.\\n    input_ids = []\\n    attention_masks = []\\n    # Encode input string(s).\\n    if type(inputs)==list:\\n        if backend==\\'multiprocessing\\': # Multiprocessing is faster than loky in processing large objects.\\n            encoded_outputs=Parallel(n_jobs=n_jobs, backend=\"multiprocessing\", batch_size=\\'auto\\', verbose=1)(delayed(func_encode_string)(text_string) for text_string in inputs)\\n            for encoded_output in encoded_outputs:\\n                # Add the encoded sentence to the list.\\n                input_ids.append(encoded_output[\\'input_ids\\'])\\n                # And its attention mask (simply differentiates padding from non-padding).\\n                attention_masks.append(encoded_output[\\'attention_mask\\'])\\n        elif backend==\\'sequential\\':\\n            for text_string in tqdm(inputs):\\n                encoded_output=func_encode_string(text_string)\\n                # Add the encoded sentence to the list.\\n                input_ids.append(encoded_output[\\'input_ids\\'])\\n                # And its attention mask (simply differentiates padding from non-padding).\\n                attention_masks.append(encoded_output[\\'attention_mask\\'])\\n        elif backend==\\'dask\\':\\n            with joblib.parallel_backend(\\'dask\\'):\\n                n_jobs=len(client.scheduler_info()[\\'workers\\']) # Get # works.\\n                string_chunks = partition_all(math.ceil(len(inputs)/n_jobs), inputs)  # Collect into groups of size 1000\\n                encoded_outputs=Parallel(n_jobs=-1, batch_size=\\'auto\\', verbose=1)(delayed(func_encode_string_batch)(text_strings) for text_strings in string_chunks)\\n                encoded_outputs=itertools.chain(*encoded_outputs)\\n            for encoded_output in encoded_outputs:\\n                # Add the encoded sentence to the list.\\n                input_ids.append(encoded_output[\\'input_ids\\'])\\n                # And its attention mask (simply differentiates padding from non-padding).\\n                attention_masks.append(encoded_output[\\'attention_mask\\'])           \\n    if type(inputs)==str:\\n        encoded_output=func_encode_string(inputs)\\n        input_ids=[encoded_output[\\'input_ids\\']]\\n        attention_masks=[encoded_output[\\'attention_mask\\']]\\n\\n    # Convert the lists into tensors.\\n    input_ids = torch.cat(input_ids, dim=0)\\n    attention_masks = torch.cat(attention_masks, dim=0)\\n\\n    # Prepare dataloader for efficient calculation.\\n    batch_size = 64\\n    pred_data = TensorDataset(input_ids, attention_masks)\\n    pred_sampler = SequentialSampler(pred_data)\\n    pred_dataloader = DataLoader(pred_data, sampler=pred_sampler, batch_size=batch_size)\\n\\n    # Start prediction.\\n    model_loaded.eval()\\n    logits_all=[]\\n    print(\\'Predicting categories ...\\')\\n    sleep(.5) # Pause a second for better printing results.\\n    for batch in tqdm(pred_dataloader, mininterval=10):\\n        # Add batch to the pre-chosen device\\n        batch = tuple(t.to(device) for t in batch)\\n        b_input_ids, b_input_mask = batch\\n        with torch.no_grad():\\n            outputs = model_loaded(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\\n        logits_all+=outputs[0].tolist()\\n\\n    # Calculate probabilities of logitcs.\\n    logits_prob=tf.nn.sigmoid(logits_all).numpy().tolist()\\n    # Find the positions of max values in logits.\\n    logits_max=np.argmax(logits_prob, axis=1)\\n    # Transfer to labels.\\n    logits_labels=label_encoder.inverse_transform(logits_max)\\n    \\n    # Compile results to be returned.\\n    result_list=[]\\n    for list_index in range(0, len(logits_labels)):\\n        result_dict={}\\n        result_dict[\\'recommended\\']=logits_labels[list_index]\\n        conf_prob=logits_prob[list_index][logits_max[list_index]]\\n        if conf_prob>=.99:\\n            result_dict[\\'confidence\\']=\\'high (>=.99)\\'\\n        elif conf_prob>=.95:\\n            result_dict[\\'confidence\\']=\\'medium (<.99|>=.95)\\'\\n        else:\\n            result_dict[\\'confidence\\']=\\'low (<.95)\\'\\n        prob_dict={}\\n        for label_index in range(0, len(label_encoder.classes_)):\\n            prob_dict[label_encoder.classes_[label_index]]=logits_prob[list_index][label_index]\\n        result_dict[\\'probabilities\\']=prob_dict\\n        result_list+=[result_dict]\\n\\n    return result_list'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get('https://raw.githubusercontent.com/ma-ji/npo_classifier/master/API/npoclass.py').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZk4LM6k7mfd"
   },
   "outputs": [],
   "source": [
    "df_UCF_eval=pd.read_pickle('../dataset/UCF/test/df_ucf_test.pkl.gz')\n",
    "df_UCF_eval['input']= df_UCF_eval['TAXPAYER_NAME']+' '+df_UCF_eval['mission_spellchk']+' '+df_UCF_eval['prgrm_dsc_spellchk']\n",
    "\n",
    "# Code as 10 broad categories.\n",
    "broad_cat_dict={'I': ['A'],\n",
    "                'II': ['B'],\n",
    "                'III': ['C', 'D'],\n",
    "                'IV': ['E', 'F', 'G', 'H'],\n",
    "                'V': ['I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'],\n",
    "                'VI': ['Q'],\n",
    "                'VII': ['R', 'S', 'T', 'U', 'V', 'W'],\n",
    "                'VIII': ['X'],\n",
    "                'IX': ['Y'],\n",
    "                'X': ['Z'],\n",
    "               }\n",
    "def ntee2cat(string):\n",
    "    global broad_cat_dict\n",
    "    return [s for s in broad_cat_dict.keys() if string in broad_cat_dict[s]][0]\n",
    "\n",
    "df_UCF_eval['broad_cat']=df_UCF_eval['NTEE1'].apply(ntee2cat)\n",
    "\n",
    "# Create sentence and encoded label lists\n",
    "sentences = df_UCF_eval.input.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available. Using GPU: Quadro RTX 6000\n",
      "Encoding inputs ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend MultiprocessingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done 312 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 9720 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=4)]: Done 25720 tasks      | elapsed:   16.9s\n",
      "[Parallel(n_jobs=4)]: Done 38607 out of 38607 | elapsed:   24.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting categories ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 604/604 [03:05<00:00,  3.26it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_results=npoclass(sentences, model_path='../../npoclass_model_bc/', backend='multiprocessing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          I     0.9220    0.9170    0.9903    0.9195    0.9530    0.9015      4291\n",
      "         II     0.9145    0.9084    0.9831    0.9114    0.9450    0.8863      6419\n",
      "        III     0.8968    0.9151    0.9947    0.9059    0.9541    0.9030      1861\n",
      "         IV     0.8989    0.8847    0.9874    0.8917    0.9347    0.8646      4329\n",
      "         IX     0.9091    0.9353    0.9957    0.9221    0.9650    0.9257      1701\n",
      "          V     0.9034    0.9176    0.9572    0.9105    0.9372    0.8749     11723\n",
      "         VI     0.6742    0.6835    0.9962    0.6788    0.8252    0.6596       436\n",
      "        VII     0.9047    0.8822    0.9803    0.8933    0.9300    0.8564      6749\n",
      "       VIII     0.8166    0.8352    0.9945    0.8258    0.9114    0.8173      1098\n",
      "\n",
      "avg / total     0.9019    0.9018    0.9776    0.9018    0.9387    0.8749     38607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.metrics import classification_report_imbalanced\n",
    "print(classification_report_imbalanced(y_true=df_UCF_eval.broad_cat, y_pred=[s['recommended'] for s in eval_results], digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original test scores (READ ONLY)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_SqyKYlA7mf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available. Using GPU: Quadro RTX 6000\n",
      "Encoding inputs ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38607/38607 [01:31<00:00, 419.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting categories ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1207/1207 [03:15<00:00,  6.17it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_results=npoclass(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          I     0.9220    0.9170    0.9903    0.9195    0.9530    0.9015      4291\n",
      "         II     0.9145    0.9084    0.9831    0.9114    0.9450    0.8863      6419\n",
      "        III     0.8968    0.9151    0.9947    0.9059    0.9541    0.9030      1861\n",
      "         IV     0.8989    0.8847    0.9874    0.8917    0.9347    0.8646      4329\n",
      "         IX     0.9091    0.9353    0.9957    0.9221    0.9650    0.9257      1701\n",
      "          V     0.9034    0.9176    0.9572    0.9105    0.9372    0.8749     11723\n",
      "         VI     0.6742    0.6835    0.9962    0.6788    0.8252    0.6596       436\n",
      "        VII     0.9047    0.8822    0.9803    0.8933    0.9300    0.8564      6749\n",
      "       VIII     0.8166    0.8352    0.9945    0.8258    0.9114    0.8173      1098\n",
      "\n",
      "avg / total     0.9019    0.9018    0.9776    0.9018    0.9387    0.8749     38607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.metrics import classification_report_imbalanced\n",
    "print(classification_report_imbalanced(y_true=df_UCF_eval.broad_cat, y_pred=[s['recommended'] for s in eval_results], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Bert_API.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
